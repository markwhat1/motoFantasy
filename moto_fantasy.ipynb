{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import pygsheets\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MotocrossFantasy.com variables and URLs\n",
    "series = 'sx'\n",
    "leagueID = 4370\n",
    "username = 'markwhat'\n",
    "password = 'bea1+9-@oD4YBKE7sdbX'\n",
    "\n",
    "mf_url_base = 'https://www.motocrossfantasy.com'\n",
    "mf_url_status = f\"{mf_url_base}/user/team-status\"\n",
    "mf_url_team_standings = f\"{mf_url_base}/user/bench-racing-divisions/{leagueID}\"\n",
    "mf_url_week_standings = f\"{mf_url_base}/user/weekly-standings/{leagueID}\"\n",
    "mf_url_race_results = f\"{mf_url_base}/user/race-results\"\n",
    "mf_url_top_picks = f\"{mf_url_base}/user/top-picks/2020-SX\"\n",
    "\n",
    "# Live timing JSON URL\n",
    "live_url = f\"http://americanmotocrosslive.com/xml/{series.lower()}/RaceResults.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_master():\n",
    "    payload = {'login_username': username, 'login_password': password, 'login': 'true'}\n",
    "\n",
    "    # Use 'with' to ensure the session context is closed after use.\n",
    "    with requests.Session() as s:\n",
    "        s.post(mf_url_base, data=payload)\n",
    "\n",
    "        # Get current week header from status page to see if update required\n",
    "        html = s.get(mf_url_status).content\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        status_header = soup.h3.get_text()\n",
    "        status = status_header.split(': ', 1)[1]\n",
    "        print(f'\"{status}\" is the current status.')\n",
    "\n",
    "        # Check if status is the same or if rider lists need to be updated\n",
    "        if check_sheets_update(status):\n",
    "            riders = mf_rider_tables(s)\n",
    "            rider_list_to_sheets(riders)\n",
    "        else:\n",
    "            riders = pd.read_csv('rider_lists.csv')\n",
    "    return riders\n",
    "\n",
    "\n",
    "def mf_find_table_urls(ses):\n",
    "    \"\"\"\n",
    "    Returns rider table list from 'motocrossfantasy.com'\n",
    "    division = 450 or 250 only\n",
    "    \"\"\"\n",
    "    html = ses.get(mf_url_status).content\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_urls = []\n",
    "    for link in soup.find_all('a', href=re.compile('pick-riders')):\n",
    "        table_urls.append(link['href'])\n",
    "\n",
    "    divisions = [450, 250]\n",
    "    div_dict = {k: v for (k, v) in zip(divisions, table_urls)}\n",
    "    return div_dict\n",
    "\n",
    "\n",
    "def mf_rider_tables(ses):\n",
    "    rider_urls = mf_find_table_urls(ses)\n",
    "\n",
    "    rider_lists = []\n",
    "    for div in rider_urls.keys():\n",
    "        html = ses.get(rider_urls.get(div)).content\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        table = soup.find('table')\n",
    "\n",
    "        # Use prettify() to feed html into pandas\n",
    "        rider_tables = pd.read_html(table.prettify(), flavor='bs4')\n",
    "\n",
    "        # Print number of tables found\n",
    "        print(f\"{len(rider_tables)} {div} class table(s) were found!\")\n",
    "\n",
    "        # read_html returns list of DataFrames, only need first one\n",
    "        df_table = rider_tables[0]\n",
    "\n",
    "        # Add Class column to beginning of DataFrame\n",
    "        df_table.insert(0, \"Class\", div, allow_duplicates=True)\n",
    "\n",
    "        # Merge rider lists\n",
    "        rider_lists.append(df_table)\n",
    "\n",
    "    # Combine DataFrames into one table for processing\n",
    "    df_riders = pd.concat(rider_lists, ignore_index=True)\n",
    "\n",
    "    # Drop blank column in position [1]; axis=1 means columns\n",
    "    df_riders.drop(df_riders.columns[1], axis=1, inplace=True)\n",
    "\n",
    "    # Rename columns to logical headers\n",
    "    cols = ['Class', 'Name', 'HC', 'LF', 'UD']\n",
    "\n",
    "    if len(df_riders.columns) == 5:\n",
    "        df_riders.columns = cols\n",
    "        df_riders['Name'] = format_name(df_riders['Name'])\n",
    "    else:\n",
    "        print(\"Rider columns could not be found.\")\n",
    "\n",
    "    df_riders.to_csv('rider_lists.csv', index=False)\n",
    "    return df_riders\n",
    "\n",
    "\n",
    "def check_sheets_update(status):\n",
    "    client = pygsheets.authorize(no_cache=True)\n",
    "    ss = client.open('2020 fantasy supercross')\n",
    "\n",
    "    wks = ss.worksheet_by_title('update')\n",
    "    old_status = wks.get_value('A1')\n",
    "    if old_status == status:\n",
    "#         print('No rider list update required. Loading CSV data.')\n",
    "        update_data = False\n",
    "        pass\n",
    "    else:\n",
    "        print('Rider list update will be performed.')\n",
    "        update_data = True\n",
    "        wks.update_value('A1', status)\n",
    "    return update_data\n",
    "\n",
    "\n",
    "def rider_list_to_sheets(rider_list):\n",
    "    client = pygsheets.authorize(no_cache=True)\n",
    "    ss = client.open('2020 fantasy supercross')\n",
    "\n",
    "    wks = ss.worksheet_by_title('rider_list')\n",
    "    wks.clear(start='B1')\n",
    "    wks.set_dataframe(rider_list, (3, 1))\n",
    "\n",
    "    # df_450 = rider_list[rider_list['Class'] == 450]\n",
    "    # df_250 = rider_list[rider_list['Class'] == 250]\n",
    "    #\n",
    "    # wks_450 = ss.worksheet_by_title('450_riders')\n",
    "    # wks_450.clear()\n",
    "    # wks_450.set_dataframe(df_450, (1, 1))\n",
    "    #\n",
    "    # wks_250 = ss.worksheet_by_title('250_riders')\n",
    "    # wks_250.clear()\n",
    "    # wks_250.set_dataframe(df_250, (1, 1))\n",
    "    return\n",
    "\n",
    "\n",
    "def get_live_timing():\n",
    "    r = requests.get(live_url)\n",
    "    live_timing = json.loads(r.text)\n",
    "\n",
    "    column_names = {'A': 'pos', 'F': 'name', 'N': 'num', 'L': 'laps', 'G': 'gap', 'D': 'diff', 'BL': 'bestlap',\n",
    "                    'LL': 'lastlap', 'S': 'status'}\n",
    "\n",
    "    df_live_timing = pd.DataFrame.from_records(live_timing['B'], columns=list(column_names.keys()))\n",
    "    df_live_timing.rename(columns=column_names, inplace=True)\n",
    "    df_live_timing['name'] = format_name(df_live_timing['name'])\n",
    "\n",
    "    # Save live timing DataFrame to CSV\n",
    "    df_live_timing.to_csv('live_timing.csv', index=False)\n",
    "\n",
    "    # Assemble current race information\n",
    "    status = live_timing['A']\n",
    "    location = live_timing['T']  # Race location/name - 'Washougal'\n",
    "    if series == 'sx':\n",
    "        long_moto_name = live_timing['S']\n",
    "        short_moto_name = live_timing['S']\n",
    "        division = live_timing['S']\n",
    "    else:\n",
    "        long_moto_name = live_timing['S'].split(' (', 1)[0]  # '450 Class Moto #2'\n",
    "        short_moto_name = long_moto_name.split('Class ', 1)[1]\n",
    "        division = long_moto_name.split('Class ', 1)[0]\n",
    "\n",
    "    client = pygsheets.authorize(no_cache=True)\n",
    "    ss = client.open('2020 fantasy supercross')\n",
    "    wks = ss.worksheet_by_title('live_timing')\n",
    "\n",
    "    # Updates to current race information\n",
    "    wks.update_values('A1:D1', [[series.upper(), location, division, short_moto_name]])\n",
    "    \n",
    "    \n",
    "\n",
    "    # Update live timing table beneath current race information\n",
    "#     wks.set_dataframe(df_live_timing, (3, 1))  # Live timing table\n",
    "    return df_live_timing\n",
    "\n",
    "\n",
    "def comb_live_timing_to_sheets():\n",
    "    df_live = get_live_timing()\n",
    "    df_rider = mf_master()\n",
    "    \n",
    "    # Keep only needed columns from rider lists\n",
    "    df_rider = df_rider[['Name', 'HC', 'UD']]\n",
    "    df_rider['Name']= df_rider['Name'].str.replace('McAdoo', 'Mcadoo') \n",
    "    df_rider['Name']= df_rider['Name'].str.replace('DeCotis', 'Decotis') \n",
    "\n",
    "\n",
    "    # Merge LiveTiming and rider lists on name columns\n",
    "    # Left keeps all rows from live_timing, even if no matches found\n",
    "    df = df_live.merge(df_rider, how='left', left_on='name', right_on='Name')\n",
    "\n",
    "    # Calc adjusted position, then set any 0 values to 1 as you can't finish less than 1\n",
    "    df['adj_pos'] = df['pos'] - df['HC']\n",
    "    df['adj_pos'] = df.adj_pos.mask(df.adj_pos <= 0, 1)\n",
    "    df = df.fillna(0, downcast='infer')\n",
    "\n",
    "    # Create points dictionary and then map adj_pos values to each point total\n",
    "    points = create_pts_dict()\n",
    "    df['pts_normal'] = df['adj_pos'].map(points['normal'])\n",
    "    df['pts_udog'] = df['adj_pos'].map(points['udog'])\n",
    "\n",
    "    # When these filters are true, don't change values; if not true, set value to 0\n",
    "    filter1 = df['UD'] == 'Yes'\n",
    "    filter2 = df['adj_pos'] <= 10\n",
    "    df['pts_udog'] = df.pts_udog.where(filter1 & filter2, 0)\n",
    "\n",
    "    # Find max points between udog and normal point totals, then drop unused columns\n",
    "    df['pts'] = df[['pts_normal', 'pts_udog']].max(axis=1)\n",
    "    df = df.drop(['Name', 'pts_normal', 'pts_udog', 'adj_pos'], axis=1)  # \n",
    "    df = df.fillna(0, downcast='infer')\n",
    "\n",
    "#     df.sort_values(by=['pts', 'name'], ascending=[False, True])\n",
    "    df.style.hide_index()\n",
    "\n",
    "    # Upload combined LiveTiming dataframe to Google Sheets\n",
    "    client = pygsheets.authorize(no_cache=True)\n",
    "    ss = client.open('2020 fantasy supercross')\n",
    "    wks = ss.worksheet_by_title('live_timing')\n",
    "    wks.set_dataframe(df, (3, 1))  # Live timing table\n",
    "    return df.style.hide_index()\n",
    "\n",
    "\n",
    "\n",
    "def format_name(df_column):\n",
    "    \"\"\"\n",
    "    df_column: DataSeries\n",
    "    \"\"\"\n",
    "    df = pd.Series(df_column).to_frame(name='name')\n",
    "    splits = df['name'].str.split(' ')\n",
    "    df['last'] = splits.str[1]\n",
    "    df['first'] = splits.str[0]\n",
    "    df['first'] = df['first'].str.slice(0, 1) + str('.')\n",
    "    df['name'] = df['last'].str.cat(df['first'], sep=', ')\n",
    "    df = df.loc[:, 'name']\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_pts_dict():\n",
    "    # Create pos/points dictionaries\n",
    "    if series == 'sx':\n",
    "        pts = [26, 23, 21, 19, 18, 17, 16, 15, 14, 13,  # 1-10\n",
    "               12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]   # 11-22\n",
    "    else:\n",
    "        pts = [25, 22, 20, 18, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3,   # 1-18\n",
    "               2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  # 19-40\n",
    "\n",
    "    dict_pos = dict(zip(range(1, len(pts) + 1), pts))\n",
    "    dict_pos_udog = dict_pos.copy()\n",
    "    for x in dict_pos_udog.keys():\n",
    "        if x <= 10:\n",
    "            dict_pos_udog[x] *= 2\n",
    "    dict_pts = dict()\n",
    "    dict_pts['normal'] = dict_pos\n",
    "    dict_pts['udog'] = dict_pos_udog\n",
    "    return dict_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "while x < 75:\n",
    "    print(f\"Downloading live timing data, update #{x}.\")\n",
    "    comb_live_timing_to_sheets()\n",
    "    time.sleep(30)\n",
    "    x += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
